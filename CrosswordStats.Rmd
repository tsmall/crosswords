---
title: "Nerdy Crossword Statistics"
author: "Tom Small III"
output:
  html_document:
    code_folding: hide
    df_print: kable
    theme: yeti
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

Teresa and I are avid New York Times crossword people. It kind of happened by accident. We started solving crosswords for fun together to pass the time. We eventually tried the NY Times's crosswords, and quickly realized how much better they are than everyone else's. So we subscribed, and now we do one every morning with our coffee.

We watched [*Wordplay*](https://en.wikipedia.org/wiki/Wordplay_%28film%29) and laughed at the notebooks people in it kept of their completion times. Well, I guess the laugh's on us. This is our notebook. And since I'm a programmer, it's even nerdier: our notebook has built-in analysis.

## Colophon

This document is written in a literate programing style, in the [R programming language][RLANG], using RStudio's [R Markdown][RMARK] feature. It uses a few great libraries from [the tidyverse][TIDYV].

  [RLANG]: https://www.r-project.org/
  [RMARK]: http://rmarkdown.rstudio.com/
  [TIDYV]: http://tidyverse.org/

```{r, message=FALSE, warning=FALSE}
library(lubridate)
library(tidyverse)
```

## Source data

The source data is how long it took us to complete each day's puzzle. The NY Times doesn't provide an API to get this data. Plus I like having my own local copy. So every day after we finish the puzzle I type our completion times into a CSV file.

I have one CSV file for each year since we started tracking. Here I load all of them into a single variable (`completion_times`) and parse the date.

```{r, message=FALSE, warning=FALSE}
times_for_2017 <- read_csv(file = "2017.csv")
times_for_2018 <- read_csv(file = "2018.csv")
times_for_2019 <- read_csv(file = "2019.csv")

completion_times <- rbind(
    times_for_2017,
    times_for_2018,
    times_for_2019
  ) %>%
  mutate(
    Weekday = wday(Date, label = TRUE),
    Month = month(Date, label = TRUE),
    Year = year(Date)
  )
```

Here are the last seven days' worth of data.

```{r, message=FALSE, warning=FALSE}
completion_times %>%
  select(Date, Weekday, Time) %>%
  tail(n = 7)
```

## Analysis

Now that we have our raw data we can start the fun part: analyzing it.

### Times by day

We'll start with an obvious question: for each day of the week, what is our average, fastest, and slowest time?

```{r, message=FALSE, warning=FALSE}
times_by_day <- completion_times %>%
  group_by(Weekday) %>%
  summarise(
    Average = hms::as.hms(round(mean(Time))),
    Fastest = min(Time),
    Slowest = max(Time)
  )

times_by_day
```

Here you can see our average times graphically:

```{r, message=FALSE, warning=FALSE}
ggplot(times_by_day) +
  geom_bar(
    mapping = aes(x = Weekday, y = Average),
    stat = "identity"
  )
```

And this is a visual representation of the variation in our times:

```{r, message=FALSE, warning=FALSE}
ggplot(completion_times) +
  geom_boxplot(
    mapping = aes(x = Weekday, y = Time)
  )
```

### Percentiles

Averages can be heavily skewed by outliers. Percentiles can help give a clearer picture of a large set of data points. Here are the 85th, 75th, and 50th percentiles for the completion dates.

```{r, message=FALSE, warning=FALSE}
completion_times %>%
  group_by(Weekday) %>%
  summarise(
    "85th" = hms::as.hms(round(quantile(Time, probs = 0.85))),
    "75th" = hms::as.hms(round(quantile(Time, probs = 0.75))),
    "50th" = hms::as.hms(round(quantile(Time, probs = 0.5)))
  )
```

### By day, by month

We're also curious how our times change from month to month. Our hypothesis is that they'll get longer every month of the year, from January to December, because the puzzles themselves will get harder.

```{r, message=FALSE, warning=FALSE}
times_by_month <- completion_times %>%
  group_by(Month, Weekday) %>%
  summarise(
    Average = hms::as.hms(round(mean(Time)))
  )

ggplot(times_by_month) +
  geom_bar(
    mapping = aes(x = Weekday, y = Average, fill = Month),
    position = "dodge",
    stat = "identity"
  )
```

It's tough to tell so far if that's actually true. It's true for some of the days, but not all of them. We'll have to wait and see what happens over the course of the year to have a better answer to the question.

### Year over year

```{r, message=FALSE, warning=FALSE}
times_by_weekday_year <- completion_times %>%
  group_by(Weekday, Year) %>%
  summarise(Average = hms::as.hms(round(mean(Time))))

times_by_weekday_year$Year <- factor(times_by_weekday_year$Year)

ggplot(times_by_weekday_year) +
  geom_bar(
    mapping = aes(x = Weekday, y = Average, fill = Year),
    position = "dodge",
    stat = "identity"
  )
```

```{r, message=FALSE, warning=FALSE}
times_by_month_year <- completion_times %>%
  group_by(Month, Year) %>%
  summarise(Average = hms::as.hms(round(mean(Time))))

times_by_month_year$Year <- factor(times_by_month_year$Year)

ggplot(times_by_month_year) +
  geom_bar(
    mapping = aes(x = Month, y = Average, fill = Year),
    position = "dodge",
    stat = "identity"
  )
```

### Over time

```{r, message=FALSE, warning=FALSE}
ggplot(completion_times, mapping = aes(x = Date, y = Time, color = Weekday)) +
  geom_point() +
  geom_smooth(se = FALSE)
```
